* Linkliste

** Shaply?! Formen finden? 
** Graph f√ºr Zurordnung

   https://peerj.com/articles/cs-1260/
   https://apps.london.gov.uk/street-trees/
   
** Protomap

   https://wasmlabs.dev/articles/maps-service-with-protomaps-wasm-workers-server/
   
** Visual Analytics for eXplainable Deep Learning

   - [[https://onlinelibrary.wiley.com/doi/10.1111/cgf.14733][Visual Analytics for eXplainable Deep Learning]]

[[./images/cgf14733-fig-0009-m.jpg]]
   
Details are in the caption following the image Figure 9 Open in figure
viewer PowerPoint Examples of how VA systems support explanations by
examples. (a) The Melody system [CBN*20] visualizes explanations by
examples for the current image by highlighting regions similar to the
most important one of the input. In the case shown, the model
predicted a blue-winged warbler since its peck and neck are similar to
the features of other blue-winged warblers. (b) Explanations by
examples for text data. The task is to help users to understand
machine translations of Seq2Seq models. The system visualizes the
hidden states of the input sequence as a trajectory, where dots in the
plane represent similar states. The user can select vertices on the
graph, and then the system will show a list of sentences that produces
similar states to the selected ones as explanations by example. The
colours (blue and yellow) indicate whether the similar states come
from the encoder or decoder, while the word that produced the similar
state is highlighted in red.

Illustrative example. One use case of Melody (Figure 9a) is to
understand a DL model for image classification. When the expert
involved in the experiment has removed clusters too small or with too
low explanation values, he has discovered three broad groups of birds
with similar prediction logics but different visual explanatory
features. By selecting one instance cluster, he has understood how the
network classifies some birds by first looking at coarse-level
features (colour) and then more detailed ones (head or belly). He has
also identified wrongly classified birds and has observed that it is
because they share similar features to other classes. This shows how
the application can help the expert to discover some parts of the
reasoning process of the network.

Summary. Despite its ease of implementation and use, this category of
explanations is not widely adopted yet and is mainly used as a
complementary tool for other types of explanations. The only
exceptions are VA systems that support self-explainable DNNs based on
prototypes, where explanations by examples are used to guide the user
to specify alternative prototypes or modify them when they cannot
satisfactorily represent their nearby samples [MXC*20]. While in
almost all the other cases, explanations by examples are valid only
for the current input (i.e. they are local), in these cases, they can
be used to get insights into the global behaviour captured by the
model. They can also be exploited to improve the design of the model
themselves by letting the user to specify the nearby desired
explanation by examples. Then the system generates a new prototype
that includes the desired samples as neighbours [MXC*20]. We do not
observe noticeable complex novel visual solutions for this category,
and most of the effort is directed towards the post hoc algorithms
needed to extract them. The customization is very limited, often only
to the sorting mechanism and rarely to the selection of the number of
explanations to visualize [JTH*21].

** Geolocation aus Bildern auslesen

   https://github.com/9p4/code-stuff/tree/master/Projects/Apps/PicFinder/Python_PicFinder

** Karten im SVG-Format

   https://cartosvg.com/#

** Vison Models

   https://github.com/voxel51/fiftyone-examples
   https://docs.voxel51.com/recipes/index.html

   
